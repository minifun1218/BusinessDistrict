#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«åŠŸèƒ½æµ‹è¯•è„šæœ¬
"""

import os
import sys
sys.path.insert(0, os.path.dirname(__file__))

from app import create_app
from app.extensions import db
from app.models.city import City
from app.crawler.crawler_manager import CrawlerManager

def test_crawler_basic():
    """æµ‹è¯•çˆ¬è™«åŸºæœ¬åŠŸèƒ½"""
    print("=== æµ‹è¯•çˆ¬è™«åŸºæœ¬åŠŸèƒ½ ===")
    
    app = create_app()
    
    with app.app_context():
        # åˆå§‹åŒ–çˆ¬è™«ç®¡ç†å™¨
        baidu_key = app.config.get('BAIDU_MAP_AK')
        amap_key = app.config.get('AMAP_KEY')
        
        with CrawlerManager(baidu_key, amap_key) as crawler_manager:
            # æµ‹è¯•çˆ¬è™«è¿é€šæ€§
            print("1. æµ‹è¯•çˆ¬è™«è¿é€šæ€§...")
            results = crawler_manager.test_crawlers()
            
            for crawler_name, result in results.items():
                status = "âœ…" if result['status'] == 'ok' else "âŒ"
                print(f"   {status} {crawler_name}: {result['status']}")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            print("\n2. è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯...")
            stats = crawler_manager.get_crawler_stats()
            print(f"   å¯ç”¨çˆ¬è™«: {', '.join(stats['available_crawlers'])}")
            
            return True

def test_crawler_with_mock_city():
    """ä½¿ç”¨æ¨¡æ‹ŸåŸå¸‚æµ‹è¯•çˆ¬è™«"""
    print("\n=== æµ‹è¯•çˆ¬è™«æ•°æ®çˆ¬å– ===")
    
    app = create_app()
    
    with app.app_context():
        # ç¡®ä¿æœ‰æµ‹è¯•åŸå¸‚æ•°æ®
        test_city = City.query.filter_by(name='åŒ—äº¬').first()
        if not test_city:
            test_city = City(
                id='110000',
                name='åŒ—äº¬',
                code='BJ',
                level='city',
                longitude=116.4074,
                latitude=39.9042,
                is_hot=True,
                pinyin='beijing',
                pinyin_abbr='bj'
            )
            db.session.add(test_city)
            db.session.commit()
            print("   åˆ›å»ºæµ‹è¯•åŸå¸‚: åŒ—äº¬")
        
        # åˆå§‹åŒ–çˆ¬è™«ç®¡ç†å™¨
        baidu_key = app.config.get('BAIDU_MAP_AK')
        amap_key = app.config.get('AMAP_KEY')
        
        with CrawlerManager(baidu_key, amap_key) as crawler_manager:
            print("1. æµ‹è¯•çˆ¬å–åŸå¸‚æ•°æ®...")
            
            # åªä½¿ç”¨å¤§ä¼—ç‚¹è¯„çˆ¬è™«ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰è¿›è¡Œæµ‹è¯•
            result = crawler_manager.crawl_city_data(
                city_id=test_city.id,
                city_name=test_city.name,
                crawlers=['dianping'],
                update_existing=True
            )
            
            if result['success']:
                print(f"   âœ… çˆ¬å–æˆåŠŸ!")
                print(f"   å•†åœˆæ•°é‡: {result['areas_count']}")
                print(f"   åº—é“ºæ•°é‡: {result['stores_count']}")
                print(f"   ä½¿ç”¨çˆ¬è™«: {', '.join(result['crawlers_used'])}")
                return True
            else:
                print(f"   âŒ çˆ¬å–å¤±è´¥: {result.get('error')}")
                return False

def test_api_endpoints():
    """æµ‹è¯•APIæ¥å£"""
    print("\n=== æµ‹è¯•APIæ¥å£ ===")
    
    app = create_app()
    client = app.test_client()
    
    with app.app_context():
        # æµ‹è¯•çˆ¬è™«çŠ¶æ€æ¥å£
        print("1. æµ‹è¯•çˆ¬è™«çŠ¶æ€æ¥å£...")
        response = client.get('/api/crawler/status')
        
        if response.status_code == 200:
            print("   âœ… çŠ¶æ€æ¥å£æ­£å¸¸")
            data = response.get_json()
            print(f"   å¯ç”¨çˆ¬è™«: {', '.join(data['data']['available_crawlers'])}")
        else:
            print(f"   âŒ çŠ¶æ€æ¥å£å¼‚å¸¸: {response.status_code}")
        
        # æµ‹è¯•çˆ¬è™«æµ‹è¯•æ¥å£
        print("\n2. æµ‹è¯•çˆ¬è™«æµ‹è¯•æ¥å£...")
        response = client.get('/api/crawler/test')
        
        if response.status_code == 200:
            print("   âœ… æµ‹è¯•æ¥å£æ­£å¸¸")
        else:
            print(f"   âŒ æµ‹è¯•æ¥å£å¼‚å¸¸: {response.status_code}")
        
        # æµ‹è¯•æ•°æ®è´¨é‡æ¥å£
        print("\n3. æµ‹è¯•æ•°æ®è´¨é‡æ¥å£...")
        response = client.get('/api/crawler/data-quality')
        
        if response.status_code == 200:
            print("   âœ… æ•°æ®è´¨é‡æ¥å£æ­£å¸¸")
            data = response.get_json()
            areas_info = data['data']['areas']
            stores_info = data['data']['stores']
            print(f"   å•†åœˆæ€»æ•°: {areas_info['total']}")
            print(f"   åº—é“ºæ€»æ•°: {stores_info['total']}")
        else:
            print(f"   âŒ æ•°æ®è´¨é‡æ¥å£å¼‚å¸¸: {response.status_code}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•çˆ¬è™«ç³»ç»Ÿ...")
    
    try:
        # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
        if not test_crawler_basic():
            print("âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥")
            return False
        
        # æ•°æ®çˆ¬å–æµ‹è¯•
        if not test_crawler_with_mock_city():
            print("âŒ æ•°æ®çˆ¬å–æµ‹è¯•å¤±è´¥")
            return False
        
        # APIæ¥å£æµ‹è¯•
        test_api_endpoints()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    main()
